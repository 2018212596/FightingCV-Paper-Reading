<img src="./tmpimg/LOGO.gif" height="200" width="400"/>


# FightingCV-Paper-Reading


Hello，大家好，我是小马🚀🚀🚀

作为研究生，读论文一直都是都是一件非常**费时费脑**的事情，因为帮助大家用**5分钟**的时间就能知道某篇论文的大致内容，我会把我看过的论文做好解析分享在这里。**项目持续更新，每周至少更新三篇！**⭐⭐⭐

本项目的宗旨是🚀**让世界上没有难读的论文**🚀，论文主题包括但不限于检测、分类、分割、Backbone、多模态等等，论文来源包括但不限于最新的arXiv论文、ICCV2021、CVPR2021、MM2021。**(项目会保持持续更新，每周至少三篇)**⭐⭐⭐
 
 
（最新还更新了[【Attention、MLP、Conv、MLP、Backbone的代码复现项目】](https://github.com/xmu-xiaoma666/External-Attention-pytorch)，欢迎大家学习交流）


***
## 公众号 & 微信交流群

欢迎大家关注公众号：**FightingCV**

公众号**每天**都会进行**论文、算法和代码的干货分享**哦~


已建立**机器学习/深度学习算法/计算机视觉/多模态交流群**微信交流群！

（加不进去可以加微信：**775629340**，记得备注【**公司/学校+方向+ID**】）

**每天在群里分享一些近期的论文和解析**，欢迎大家一起**学习交流**哈~~~

![](./tmpimg/wechat.jpg)

强烈推荐大家关注[**知乎**](https://www.zhihu.com/people/jason-14-58-38/posts)账号和[**FightingCV公众号**](https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w)，可以快速了解到最新优质的干货资源。

***


## 总结性文章

- [从多篇2021年顶会论文看多模态预训练模型最新研究进展](https://zhuanlan.zhihu.com/p/425859974)  


- [深度学习中的重参数机制总结与代码实现](https://zhuanlan.zhihu.com/p/383660483)  

- [深度学习中的Attention总结（一）](https://zhuanlan.zhihu.com/p/379657870)  

- [深度学习中的Attention总结（二）](https://zhuanlan.zhihu.com/p/386333201)  

- [思考NLP和CV中的Local和Global建模](https://zhuanlan.zhihu.com/p/387766129)  

## NeurIPS2021
### Transformer


- [NeurIPS2021-《HRFormer》-HRNet又出续作啦！国科大&北大&MSRA提出高分辨率Transformer，代码已开源！](https://zhuanlan.zhihu.com/p/429936715)  
    [【HRFormer: High-Resolution Transformer for Dense Prediction】](https://arxiv.org/abs/2110.09408)

- [NeurIPS2021-ViT现在可以做目标检测任务啦！华科提出目标检测新方法YOLOS](https://zhuanlan.zhihu.com/p/4262947628)  
    [【You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection】](https://www.arxiv-vanity.com/papers/2106.00666/)

- [NeurIPS2021-没有残差连接的ViT准确率只有0.15%！！！北大&华为提出用于Vision Transformer的Augmented Shortcuts，涨点显著！](https://zhuanlan.zhihu.com/p/424214038)  
    [【Augmented Shortcuts for Vision Transformers】](https://arxiv.org/abs/2106.15941)

- [NeurIPS2021- Transformer部署难？北大&华为诺亚提出Vision Transformer的后训练量化方法](https://zhuanlan.zhihu.com/p/423936004)  
    [【Post-Training Quantization for Vision Transformer】](https://arxiv.org/abs/2106.14156)

- [Multi-Scale Densenet续作？动态ViT](https://zhuanlan.zhihu.com/p/386929227)  
    [【Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length】](https://arxiv.org/abs/2105.15075)

- [微软新作Focal Self-Attention：具备Local和Global交互能力的Transformer](https://zhuanlan.zhihu.com/p/387693270)  
    [【Focal Self-attention for Local-Global Interactions in Vision Transformers】](https://arxiv.org/abs/2107.00641)


***

### 多模态

- [NeurIPS2021-《MBT》-多模态数据怎么融合？谷歌提出基于注意力瓶颈的方法，简单高效还省计算量](https://zhuanlan.zhihu.com/p/427779731)  
    [【Attention Bottlenecks for Multimodal Fusion】](https://arxiv.org/abs/2107.00135)


- [NeurIPS2021-快来刷榜吧！微软提出新的视频多模态benchmark，同时包含检索、caption、QA等多个任务！](https://zhuanlan.zhihu.com/p/433827807)  
    [【VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation】](https://arxiv.org/abs/2106.04632)

***


### 动态网络

- [NeurIPS2021-用多大分辨率的图像做分类更适合？浙大&华为&国科大提出Dynamic Resolution Network，降低计算量还能提性能！](https://zhuanlan.zhihu.com/p/428436758)  
    [【Dynamic Resolution Network】](https://arxiv.org/abs/2106.02898)


***


## ICCV2021
### 多模态（Multi-Modal）
- [ICCV2021 Oral-MDETR：图灵奖得主Yann LeCun的团队&Facebook提出端到端多模态理解的目标检测器](https://zhuanlan.zhihu.com/p/394239659)  
    [【MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding】](https://arxiv.org/abs/2104.12763)

- [ICCV2021-NTU用多样性的query生成，涨点基于文本的实例分割（已开源）](https://zhuanlan.zhihu.com/p/404955179)  
    [【Vision-Language Transformer and Query Generation for Referring Segmentation】](https://arxiv.org/abs/2108.05565)

- [ICCV2021-如何高效视频定位？北大&Adobe&QMUL强强联手提出弱监督CRM，性能SOTA](https://zhuanlan.zhihu.com/p/406704588)  
    [【Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation】](https://arxiv.org/abs/2107.11443)
    
- [ICCV2021-TOCo-微软&CMU提出Token感知的级联对比学习方法，在视频文本对齐任务上“吊打”其他SOTA方法](https://zhuanlan.zhihu.com/p/406827017)  
    [【TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment】](https://arxiv.org/abs/2108.09980)

- [ICCV2021 Oral-新任务！新数据集！康奈尔大学提出了类似VG但又不是VG的PVG任务](https://zhuanlan.zhihu.com/p/407102211)  
    [【Who’s Waldo? Linking People Across Text and Images】](https://arxiv.org/abs/2108.07253)


- [ICCV2021-新任务！NTU&港中文提出以对话的方式进行细粒度的图片编辑](https://zhuanlan.zhihu.com/p/418089405) 
    [【Talk-to-Edit: Fine-Grained Facial Editing via Dialog】](https://arxiv.org/abs/2109.04425)



- [ICCV2021-用DETR的方法做Dense Video Captioning！港大&南科大提出端到端PDVC，简化训练流程。](https://zhuanlan.zhihu.com/p/418100751) 
    [【End-to-End Dense Video Captioning with Parallel Decoding】](https://arxiv.org/abs/2108.07781)


### 对比学习（Contrastive Learning）
- [ICCV2021-DetCo：性能优于何恺明等人提出的MoCo v2，为目标检测定制任务的对比学习。](https://zhuanlan.zhihu.com/p/393202411)  
    [【DetCo: Unsupervised Contrastive Learning for Object Detection】](https://arxiv.org/abs/2102.04803)


### 可解释性（Interpretability）
- [ICCV2021 Oral-TAU&Facebook提出了通用的Attention模型可解释性](https://zhuanlan.zhihu.com/p/394794493)  
    [【Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers】](https://arxiv.org/abs/2103.15679)

- [ICCV2021 -为什么深度学习模型能够分类正确？SCOUTER能够“正”“反”两个方面说服你。](https://zhuanlan.zhihu.com/p/396783525)  
    [【SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition】](https://arxiv.org/abs/2009.06138)


### 主干网络（Backbone，CNN，Transformer）
- [ICCV2021-iRPE-还在魔改Transformer结构吗？微软&中山大学提出超强的图片位置编码，涨点显著](https://zhuanlan.zhihu.com/p/395766591)   
    [【Rethinking and Improving Relative Position Encoding for Vision Transformer】](https://arxiv.org/abs/2107.14222)

- [ICCV2021 | 池化操作不是CNN的专属，Vision Transformer说：“我也可以”；南大提出池化视觉Transformer（PiT）](https://zhuanlan.zhihu.com/p/398763751)  
    [【Rethinking Spatial Dimensions of Vision Transformers】](https://arxiv.org/abs/2103.16302)

- [ICCV2021 | CNN+Transformer=Better，国科大&华为&鹏城实验室 出Conformer，84.1% Top-1准确率](https://zhuanlan.zhihu.com/p/400244375)  
    [【Conformer: Local Features Coupling Global Representations for Visual Recognition】](https://arxiv.org/abs/2105.03889)

- [ICCV2021 | MicroNets-更小更快更好的MicroNet，三大CV任务都秒杀MobileNetV3](https://zhuanlan.zhihu.com/p/400661708)  
    [【MicroNet: Improving Image Recognition with Extremely Low FLOPs】](https://arxiv.org/abs/2108.05894)


- [ICCV2021-MIT-IBM AI Lab开源CrossViT，Transformer开始走向多分支、多尺度（附目前多尺度ViT的异同点对比）](https://zhuanlan.zhihu.com/p/418086070)  
    [【CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification】](https://arxiv.org/abs/2103.14899)



### 多任务（Multi-Task）
- [ICCV2021-MuST-还在特定任务里为刷点而苦苦挣扎？谷歌的大佬们都已经开始玩多任务训练了](https://zhuanlan.zhihu.com/p/406014791)  
    [【Multi-Task Self-Training for Learning General Representations】](https://arxiv.org/abs/2108.11353)

- [ICCV2021-CV多任务新进展！一节更比三节强的MultiTask CenterNet，用一个网络同时完成目标检测、语义分割和人体姿态估计三个任务](https://zhuanlan.zhihu.com/p/405652732)  
    [【MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach】](https://arxiv.org/abs/2108.05060)

### 数据增强

- [ICCV 2021｜“白嫖”性能的MixMo，一种新的数据增强or模型融合方法](https://zhuanlan.zhihu.com/p/418098973)   
    [【MicroNet: Improving Image Recognition with Extremely Low FLOPs】](https://arxiv.org/abs/2108.05894)


- [ICCV2021 Oral-简单高效的数据增强！华为提出了一种简单的鲁棒目标检测自适应方法](https://zhuanlan.zhihu.com/p/396528978)   
    [【SimROD: A Simple Adaptation Method for Robust Object Detection】](https://arxiv.org/abs/2107.13389)


### 其他


- [ICCV'21 Oral｜拒绝调参，显著提点！检测分割任务的新损失函数RS Loss开源](https://zhuanlan.zhihu.com/p/397519850)  
    [【Rank & Sort Loss for Object Detection and Instance Segmentation】](https://arxiv.org/abs/2107.11669)

- [ICCV21 | 大道至简，仅需4行代码提升多标签分类性能！ 南大提出Residual Attention](https://zhuanlan.zhihu.com/p/397990353)  
    [【Residual Attention: A Simple but Effective Method for Multi-Label Recognition】](https://arxiv.org/abs/2108.02456)

- [ICCV2021 Oral-UNO-用于Novel Class Discovery 的统一目标函数，简化训练流程！已开源！](https://zhuanlan.zhihu.com/p/407365987)  
    [【A Unified Objective for Novel Class Discovery】](https://arxiv.org/abs/2108.08536)

- [ICCV2021-别魔改网络了，模型精度不高，是你Resize的方法不够好！Google提出基于DL的调整器模型学习更好的Resize方法](https://zhuanlan.zhihu.com/p/409582813)  
    [【Learning to Resize Images for Computer Vision Tasks】](https://arxiv.org/abs/2103.09950)

- [ICCV2021-《GroupFormer》-商汤&港理工提出基于聚类的联合建模时空关系的GroupFormer用于解决群体活动识别问题，性能SOTA](https://zhuanlan.zhihu.com/p/411674711)  
    [【GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer】](https://arxiv.org/abs/2108.12630)


- [ICCV2021-去除冗余token的DETR效果怎么样？NUS颜水成大佬团队给出了答案！](https://zhuanlan.zhihu.com/p/415579801)  
    [【PnP-DETR: Towards Efficient Visual Analysis with Transformers】](https://arxiv.org/abs/2109.07036)



- [ICCV2021-还在用大量数据暴力train模型？主动学习，教你选出数据集中最有价值的样本](https://zhuanlan.zhihu.com/p/420756941)  
    [【Active Learning for Deep Object Detection via Probabilistic Modeling】](https://arxiv.org/abs/2103.16130)




- [ICCV2021-比MoCo更通用的对比学习范式，中科大&MSRA提出对比学习新方法MaskCo](https://zhuanlan.zhihu.com/p/4209392131)  
    [【Self-Supervised Visual Representations Learning by Contrastive Mask Prediction】](https://arxiv.org/abs/2108.07954)


***

## ACM MM2021
### 主干网络（Backbone，CNN，Transformer）
- [ACM MM2021-还在用ViT的16x16 Patch分割方法吗？中科院自动化所提出Deformable Patch-based方法，涨点显著！](https://zhuanlan.zhihu.com/p/399417704)  
    [【DPT: Deformable Patch-based Transformer for Visual Recognition】](https://arxiv.org/abs/2107.14467)

- [ACMMM 2021-多模态宝藏！京东梅涛团队重磅开源第一个适用于多个任务的多模态代码库x-modaler！](https://zhuanlan.zhihu.com/p/403688076)  
    [【X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics】](https://arxiv.org/abs/2108.08217)

- [ACMMM 2021-性能SOTA！用GNN和GAN的方式来强化Video Captioning的学习！](https://zhuanlan.zhihu.com/p/403895573)  
    [【Discriminative Latent Semantic Graph for Video Captioning】](https://arxiv.org/abs/2108.03662)

***

## ICML2021
### 预训练（pre-train）
- [ICML2021-《ALIGN》-大力出奇迹，谷歌用18亿的图像-文本对训练了一个这样的模型。](https://zhuanlan.zhihu.com/p/410499923)  
    [【Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision】](https://zhuanlan.zhihu.com/p/410499923)

***

## CVPR2021
### 多模态（Multi-Modal）
- [Less is More-CVPR2021最佳学生论文提名](https://zhuanlan.zhihu.com/p/388824565)  
    [【Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling】](https://arxiv.org/abs/2102.06183)

- [CVPR2021-RSTNet：自适应Attention的“看图说话”模型](https://zhuanlan.zhihu.com/p/394793465)  
    [【RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Words】](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.html)

- [CVPR2021 Oral《Seeing Out of the Box》北科大&中山大学&微软提出端到端视觉语言表征预训练方法](https://zhuanlan.zhihu.com/p/395982625)   
    [【Seeing Out of the Box: End-to-End Pre-Training for Vision-Language Representation Learning】](https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Seeing_Out_of_the_Box_End-to-End_Pre-Training_for_Vision-Language_Representation_CVPR_2021_paper.html)

- [CVPR2021-开放式的Video Captioning，中科院自动化所提出基于“检索-复制-生成”的网络](https://zhuanlan.zhihu.com/p/401333569)   
    [【Open-book Video Captioning with Retrieve-Copy-Generate Network】](https://arxiv.org/abs/2103.05284)

- [CVPR2021-多模态任务新进展！哥大&Facebook提出VX2TEXT模型，实现了“视频+X”到“文本”的任务](https://zhuanlan.zhihu.com/p/403340498)   
    [【VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs】](https://arxiv.org/abs/2101.12059)

- [CVPR2021-人大提出新模型，将Two Stage的Video Paragraph Captioning变成One Stage，性能却没下降](https://zhuanlan.zhihu.com/p/404419987)   
    [【Towards Diverse Paragraph Captioning for Untrimmed Videos】](https://arxiv.org/abs/2105.14477)


- [CVPR2021-用更好的目标检测器提取视觉特征！微软提出VinVL，基于更好的视觉特征，达到更强的多模态性能。](https://zhuanlan.zhihu.com/p/422114283)   
    [【VinVL: Revisiting Visual Representations in Vision-Language Models】](https://arxiv.org/abs/2104.13682)



- [CVPR2021 Oral-不再需要后处理步骤！Kakao提出端到端的Human-Object交互检测模型](https://zhuanlan.zhihu.com/p/426929486)   
    [【HOTR: End-to-End Human-Object Interaction Detection with Transformers】](https://arxiv.org/abs/2101.00529)


### 主干网络（Backbone，CNN，Transformer）
- [谷歌新作HaloNet：Transformer一作用Self-Attention的方式进行卷积](https://zhuanlan.zhihu.com/p/388598744)  
    [【Scaling Local Self-Attention for Parameter Efficient Visual Backbones】](https://zhuanlan.zhihu.com/p/388598744)

- [Involution（附对Involution的思考）：港科大、字节跳动、北大提出“内卷”神经网络算子，在CV三大任务上提点明显](https://zhuanlan.zhihu.com/p/395950242)  
    [【Involution: Inverting the Inherence of Convolution for Visual Recognition】](https://arxiv.org/pdf/2103.06255.pdf)

- [CVPR2021-比CNN和Transformer更好的Backbone？UC Berkeley&Google Research,提出BoTNet，ImageNet上精度达84.7%](https://zhuanlan.zhihu.com/p/418096136)  
    [【Bottleneck Transformers for Visual Recognition】](https://arxiv.org/abs/2101.11605)

### 目标检测（Detection）


- [CVPR2021 Oral-收敛更快！精度更高！南科大&腾讯微信团队重磅开源无监督预训练的UP-DETR](https://zhuanlan.zhihu.com/p/419660108)  
    [【UP-DETR: Unsupervised Pre-training for Object Detection with Transformers】](https://arxiv.org/abs/2011.09094)


***


## SIGIR 2021
### 多模态（Multi-Modal）
- [SIGIR 2021 最佳学生论文-图像文本检索的动态模态交互建模](https://zhuanlan.zhihu.com/p/402122260)  
    [【Dynamic Modality Interaction Modeling for Image-Text Retrieval】](https://dl.acm.org/doi/abs/10.1145/3404835.3462829)

    
- [SimVLM-拒绝各种花里胡哨！CMU&Google提出弱监督极简VLP模型，在多个多模态任务上性能SOTA](https://zhuanlan.zhihu.com/p/406354414)  
    [【SimVLM: Simple Visual Language Model Pretraining with Weak Supervision】](https://zhuanlan.zhihu.com/p/406354414)

***



## EMNLP2021
### 多模态（Multi-Modal）
- [多模态Transformer真的多模态了吗？论多模态Transformer对跨模态的影响](https://zhuanlan.zhihu.com/p/411890653)  
    [【Vision-and-Language or Vision-for-Language? On Cross-Modal Inflfluence in Multimodal Transformers】](https://arxiv.org/abs/2109.04448)

- [EMNLP2021-“Transformer+预训练”再下一城，港科大开源高效的多模态摘要总结网络](https://zhuanlan.zhihu.com/p/418923591)  
    [【Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization】](https://arxiv.org/abs/2109.02401)

***

## TPAMI
### 压缩加速
- [TPAMI2021-华为诺亚&悉尼大学陶大程团队提出多功能卷积，助力轻量级网络](https://zhuanlan.zhihu.com/p/423130563)  
    [【Learning Versatile Convolution Filters for Efficient Visual Recognition】](https://arxiv.org/abs/2109.09310v1)

***

## ArXiv
### 主干网络（Backbone，CNN，Transformer）
- [OutLook Attention：具有局部信息感知能力的ViT](https://zhuanlan.zhihu.com/p/385561050)  
    [【VOLO: Vision Outlooker for Visual Recognition】](https://arxiv.org/abs/2106.13112)

- [CoAtNet：卷积+注意力=？？？](https://zhuanlan.zhihu.com/p/385578588)  
    [【CoAtNet: Marrying Convolution and Attention for All Data Sizes】](https://arxiv.org/abs/2106.04803)


- [CSWin-T：微软、中科大提出十字形注意力的CSWin Transformer](https://zhuanlan.zhihu.com/p/388370370)  
    [【CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows】](https://arxiv.org/abs/2107.00652)

- [Circle Kernel：清华黄高团队、康奈尔大学提出圆形卷积，进一步提升卷积结构的性能](https://zhuanlan.zhihu.com/p/389159556)  
    [【Integrating Circle Kernels into Convolutional Neural Networks】](https://arxiv.org/abs/2107.02451)

- [视觉解析器ViP：牛津大学&字节跳动提出Visual Parser，显式建模高级语义信息](https://zhuanlan.zhihu.com/p/390765725)  
    [【Visual Parser: Representing Part-whole Hierarchies with Transformers】](https://arxiv.org/abs/2107.05790)
    
- [LG-Transformer：全局和局部建模Transformer结构新作](https://zhuanlan.zhihu.com/p/393202842)  
    [【Local-to-Global Self-Attention in Vision Transformers】](https://arxiv.org/abs/2107.04735)

- [CoTNet-重磅开源！京东AI Research提出新的主干网络CoTNet,在CVPR上获得开放域图像识别竞赛冠军](https://zhuanlan.zhihu.com/p/394795481)  
    [【Contextual Transformer Networks for Visual Recognition】](https://arxiv.org/abs/2107.12292)

- [S²-MLPv2-百度提出目前最强的视觉MLP架构，超越MLP-Mixer、Swin Transformer、CycleMLP等，达到83.6% Top-1准确率](https://zhuanlan.zhihu.com/p/397003638)  
    [【S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision】](https://arxiv.org/abs/2108.01072)

- [更深和更宽的Transformer，那个比较好？NUS团队给出了给出“Go Wider Instead of Deeper”的结论](https://zhuanlan.zhihu.com/p/398168686)  
    [【Go Wider Instead of Deeper】](https://arxiv.org/abs/2107.11817)

- [在目标检测任务上怒涨8.6 AP，微软新作MobileFormer](https://zhuanlan.zhihu.com/p/400291282)  
    [【Mobile-Former: Bridging MobileNet and Transformer】](https://arxiv.org/abs/2108.05895)

- [又简单又好用的Transformer变体！清华&MSRA开源线性复杂度的Fastformer！](https://zhuanlan.zhihu.com/p/409050589)  
    [【Fastformer: Additive Attention Can Be All You Need】](https://arxiv.org/abs/2108.09084)

- [《Visformer》-对视觉任务更友好的Transformer，北航团队开源Visformer！](https://zhuanlan.zhihu.com/p/409784985)  
    [【Visformer: The Vision-friendly Transformer】](https://arxiv.org/abs/2104.12533v4)

- [《CrossFormer》-简单高效！浙大CAD&腾讯&哥大开源跨尺度的Transformer，显著涨点检测、分割、分类三大CV任务](https://zhuanlan.zhihu.com/p/410155334)  
    [【CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention】](https://arxiv.org/abs/2108.00154)


- [你见过长得像CNN的MLP吗？UO&UIUC提出了用于视觉任务的层次卷积MLP](https://zhuanlan.zhihu.com/p/418094475)  
    [【ConvMLP: Hierarchical Convolutional MLPs for Vision】](https://arxiv.org/abs/2109.04454)


- [Self-Attention真的是必要的吗？微软&中科大提出Sparse MLP，降低计算量的同时提升性能！](https://zhuanlan.zhihu.com/p/418093199)  
    [【Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?】](https://arxiv.org/abs/2109.05422)



- [目标检测再次革新！图灵奖得主Hinton团队提出Pix2Seq，将Detection变成了Image Captioning](https://zhuanlan.zhihu.com/p/418095279)  
    [【Pix2seq: A Language Modeling Framework for Object Detection】](https://arxiv.org/abs/2109.10852)



- [它来了！轻量、通用、适用于移动设备的Transformer！苹果公司提出了MobileViT](https://zhuanlan.zhihu.com/p/424669337)  
    [【MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer】](https://arxiv.org/abs/2110.02178)



- [《UFO-ViT》-Transformer可以不需要Softmax？Kakao提出了UFO-ViT，性能高，计算量还小](https://zhuanlan.zhihu.com/p/431194075)  
    [【UFO-ViT: High Performance Linear Vision Transformer without Softmax】](https://arxiv.org/abs/2109.14382)



### 分割（Segmentation）
- [MaskFormer：语义分割、实例分割“大一统”：Facebook&UIUC提出MaskFormer](https://zhuanlan.zhihu.com/p/392731360)  
    [【Per-Pixel Classification is Not All You Need for Semantic Segmentation】](https://arxiv.org/abs/2107.06278)

- [新的通道和空间注意力建模结构Polarized Self-Attention，霸榜COCO人体姿态估计和Cityscapes语义分割](https://zhuanlan.zhihu.com/p/389770482)  
    [【Polarized Self-Attention: Towards High-quality Pixel-wise Regression】](https://arxiv.org/pdf/2107.00782.pdf)


- [全景分割第一名！南大&港大&NVIDIA提出Panoptic SegFormer，霸榜全景分割](https://zhuanlan.zhihu.com/p/418088118)  
    [【Panoptic SegFormer】](https://arxiv.org/abs/2109.03814)


- [中科院&西交&旷视（孙剑团队）提出用于语义分割的动态路由网络，精确感知多尺度目标，代码已开源！](https://zhuanlan.zhihu.com/p/427709226)  
    [【Learning Dynamic Routing for Semantic Segmentation】](https://arxiv.org/abs/2003.10401)


### 检测（Detection）
- [《Anchor DETR》-加了Anchor Point能够让DETR又快又好？旷视孙剑团队提出Anchor DETR](https://zhuanlan.zhihu.com/p/411889426)  
    [【Anchor DETR: Query Design for Transformer-Based Detector】](https://arxiv.org/abs/2109.07107)



- [加了Anchor Point能够让DETR又快又好？旷视孙剑大佬团队提出Anchor DETR](https://zhuanlan.zhihu.com/p/415578473)  
    [【Anchor DETR: Query Design for Transformer-Based Detector】](https://arxiv.org/abs/2109.07107)


### 增量学习（Incremental Learning）
- [让模型实现“终生学习”，佐治亚理工学院提出Data-Free的增量学习](https://zhuanlan.zhihu.com/p/399085992)  
    [【Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning】](https://arxiv.org/abs/2106.09701)

### 多模态（Multi-Modal）
- [国科大提出用于VideoQA的跨模态交互时间金字塔Transformer](https://zhuanlan.zhihu.com/p/419923517)  
    [【Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering】](https://arxiv.org/abs/2109.04735)

- [10亿参数！别只玩GPT，来看看这个已经落地的国产模型BriVL！人大&中科院联手打造第一个大规模多模态中文预训练模型](https://zhuanlan.zhihu.com/p/425672126)  
    [【WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training】](https://arxiv.org/abs/2103.06561)


- [CLIP对视觉和语言任务有多大的好处？UC Berkeley&UCLA团队给出了答案！](https://zhuanlan.zhihu.com/p/429243265)  
    [【How Much Can CLIP Benefit Vision-and-Language Tasks?】](https://arxiv.org/abs/2107.06383)


- [消除预训练模型的语言限制！Google提出跨语言的多模态、多任务检索模型MURAL](https://zhuanlan.zhihu.com/p/418098303)  
    [【MURAL: Multimodal, Multitask Retrieval Across Languages】](https://arxiv.org/abs/2109.05125v1)

### 视频（Video）
- [Video Swin Transformer-既Swin Transformer之后，MSRA开源Video Swin Transformer，在视频数据集上SOTA](https://zhuanlan.zhihu.com/p/401600421)  
    [【Video Swin Transformer】](https://arxiv.org/abs/2106.13230)

- [基于时空混合attention的视频Transformer，大幅度降低计算复杂度](https://zhuanlan.zhihu.com/p/420280467)  
    [【Space-time Mixing Attention for Video Transformer】](https://arxiv.org/abs/2106.05968)



### 压缩加速

- [DynamicViT-还在用全部token训练ViT？清华&UCLA提出token的动态稀疏化采样，降低inference时的计算量](https://zhuanlan.zhihu.com/p/405326718)  
    [【DynamicViT: Effificient Vision Transformers with Dynamic Token Sparsifification】](https://arxiv.org/abs/2106.02034)
    

- [加速了DeiT-S 60%+的吞吐量！自动化所&上交&优图提出Evo-ViT，用Slow-Fast的方式更新token](https://zhuanlan.zhihu.com/p/412199816)  
    [【Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer】](https://arxiv.org/abs/2108.01390v3)


- [压缩之后神经网络忘记了什么？Google研究员给出了答案](https://zhuanlan.zhihu.com/p/418099910)  
    [【What Do Compressed Deep Neural Networks Forget?】](https://arxiv.org/abs/1911.05248)



### 动态网络

- [浙大&华为诺亚&西湖大学提出用于目标检测的动态特征金字塔DyFPN，减少40%的FLOPs！](https://zhuanlan.zhihu.com/p/428439288)  
    [【Dynamic Feature Pyramid Networks for Object Detection】](https://arxiv.org/abs/2012.00779)

- [《Dynamic Routing》-中科院&西交&旷视（孙剑团队）提出用于语义分割的动态路由网络，精确感知多尺度目标，代码已开源！](https://zhuanlan.zhihu.com/p/430452628)  
    [【Learning Dynamic Routing for Semantic Segmentation】](https://arxiv.org/abs/2003.10401)

- [普林斯顿大学&英伟达&Facebook提出基于深度神经网络的全动态推理，助力轻量级网络！](https://zhuanlan.zhihu.com/p/430518300)  
    [【Fully Dynamic Inference with Deep Neural Networks】](https://arxiv.org/abs/2007.15151)


### 多模态检索


- [CLIP再创辉煌！西南交大&MSRA提出CLIP4Clip，进行端到端的视频文本检索！](https://zhuanlan.zhihu.com/p/433063611)  
    [【CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval】](https://arxiv.org/abs/2104.08860)

- [腾讯PCG提出CLIP2Video，基于CLIP解决视频文本检索问题，性能SOTA！代码已开源！](https://zhuanlan.zhihu.com/p/433083355)  
    [【CLIP2Video: Mastering Video-Text Retrieval via Image CLIP】](https://arxiv.org/abs/2106.11097)


### 其他


- [拒绝Prompt Engineering，NTU提出CoOp，自适应学习不同下游任务的Prompt，性能碾压手工设计的Prompt](https://zhuanlan.zhihu.com/p/408190719)  
    [【Learning to Prompt for Vision-Language Models】](https://arxiv.org/abs/2109.01134)



- [深度神经网络其实并不需要那么深！普林斯顿大学&Intel提出ParNet，12层的网络就能达到80%以上的准确率！](https://zhuanlan.zhihu.com/p/429732072)  
    [【Non-deep Networks】](https://arxiv.org/abs/2110.07641)




- [NeurIPS2021-港大&腾讯AI Lab&牛津大学提出CARE，让CNN和Transformer能在对比学习中“互帮互助”！](https://zhuanlan.zhihu.com/p/430773996)  
    [【Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning】](https://arxiv.org/abs/2110.05340)



- [FAIR三神Kaiming，Piotr，Ross新作，MAE才是YYDS！仅用ImageNet1K，Top-1准确率87.8%，封神！](https://zhuanlan.zhihu.com/p/432663453)  
    [【Masked Autoencoders Are Scalable Vision Learners】](https://arxiv.org/abs/2111.06377)


- []()  
    [【】]()